# План интеграции и тестирования

## Стратегия: Инкрементальная интеграция

**Принцип**: Каждый PR добавляет один middleware, полностью протестирован, работает независимо.

---

## PR #1: Middleware Infrastructure (3-5 дней)

### Что делаем

```
agentarea_agents_sdk/
├── middleware/
│   ├── __init__.py
│   ├── base.py          # Protocol + MiddlewareStack
│   └── state.py         # StateBackend Protocol + InMemoryState
└── agents/
    └── stateful_agent.py  # StatefulAgent с middleware support
```

### Код

**middleware/base.py**:
```python
from typing import Protocol, Any
from ..models.llm_model import LLMResponse

class Middleware(Protocol):
    async def before_llm_call(self, state: dict) -> None: pass
    async def after_llm_call(self, state: dict, response: LLMResponse) -> None: pass
    async def before_tool_call(self, tool_call: dict, state: dict) -> dict: return tool_call
    async def after_tool_call(self, tool_call: dict, result: Any, state: dict) -> Any: return result

class MiddlewareStack:
    def __init__(self, middlewares: list[Middleware]):
        self.middlewares = middlewares

    async def run_before_llm(self, state: dict):
        for mw in self.middlewares:
            await mw.before_llm_call(state)
    # ... остальные методы
```

**middleware/state.py**:
```python
from typing import Protocol, Any

class StateBackend(Protocol):
    def get(self, key: str, default: Any = None) -> Any: ...
    def set(self, key: str, value: Any) -> None: ...
    def update(self, updates: dict) -> None: ...

class InMemoryState:
    def __init__(self):
        self._data = {}

    def get(self, key: str, default: Any = None) -> Any:
        return self._data.get(key, default)
    # ... остальные методы
```

**agents/stateful_agent.py**:
```python
class StatefulAgent:
    def __init__(self, ..., middlewares=None, state_backend=None):
        self.state = state_backend or InMemoryState()
        self.middlewares = MiddlewareStack(middlewares or [])
        # ... existing Agent code

    async def run_stream(self, task):
        # Initialize state if needed
        if not self.state.get('initialized'):
            self.state.update({
                'messages': [],
                'initialized': True
            })

        # Existing agent loop + middleware hooks
        while iteration < max_iterations:
            await self.middlewares.run_before_llm(self.state)
            # ... existing code
```

### Тесты

**tests/middleware/test_base.py**:
```python
import pytest
from agentarea_agents_sdk.middleware import Middleware, MiddlewareStack

class MockMiddleware:
    def __init__(self):
        self.before_llm_called = False
        self.after_llm_called = False

    async def before_llm_call(self, state):
        self.before_llm_called = True
        state['test'] = 'value'

    async def after_llm_call(self, state, response):
        self.after_llm_called = True

@pytest.mark.unit
async def test_middleware_stack_execution():
    mw1 = MockMiddleware()
    mw2 = MockMiddleware()
    stack = MiddlewareStack([mw1, mw2])

    state = {}
    await stack.run_before_llm(state)

    assert mw1.before_llm_called
    assert mw2.before_llm_called
    assert state['test'] == 'value'

@pytest.mark.unit
async def test_stateful_agent_initialization():
    from agentarea_agents_sdk.agents import StatefulAgent

    agent = StatefulAgent(
        name="Test",
        instruction="Test agent",
        model_provider="ollama_chat",
        model_name="qwen2.5"
    )

    assert agent.state.get('initialized') is None
    # После первого run state должен инициализироваться
```

### Как тестировать

```bash
cd Эксперименты/agentarea-agents-sdk

# Unit tests (без LLM)
pytest tests/middleware/test_base.py -m unit -v

# Integration test (с mock LLM)
pytest tests/middleware/test_stateful_agent.py -v
```

---

## PR #2: TodoListMiddleware (3-4 дня)

### Что делаем

```
agentarea_agents_sdk/
├── middleware/
│   └── todolist.py      # TodoListMiddleware
└── tools/
    └── write_todos_tool.py  # Tool для write_todos
```

### Код

**middleware/todolist.py**:
```python
from ..tasks.task_service import InMemoryTaskService
from ..tasks.tasks import TaskStatus
from uuid import UUID

class TodoListMiddleware:
    def __init__(self):
        self.task_service = InMemoryTaskService()

    async def before_llm_call(self, state: dict):
        # Inject write_todos tool if not present
        if 'write_todos_injected' not in state:
            state['write_todos_injected'] = True

    async def before_tool_call(self, tool_call: dict, state: dict):
        if tool_call['function']['name'] == 'write_todos':
            todos = tool_call['function']['arguments']['todos']

            # Sync with TaskService
            for todo in todos:
                if 'id' in todo:
                    # Update existing
                    self.task_service.set_status(
                        UUID(todo['id']),
                        TaskStatus(todo['status'])
                    )
                else:
                    # Create new
                    task = self.task_service.create(
                        title=todo['content'],
                        metadata={'activeForm': todo['activeForm']}
                    )
                    todo['id'] = str(task.id)

            # Update state
            state['todos'] = todos

            # Return mock result (tool is no-op)
            tool_call['_result'] = {
                'success': True,
                'todos_count': len(todos)
            }

        return tool_call
```

**tools/write_todos_tool.py**:
```python
from .base_tool import BaseTool

class WriteTodosTool(BaseTool):
    @property
    def name(self) -> str:
        return "write_todos"

    @property
    def description(self) -> str:
        return "Create or update the TODO list for the current task"

    def get_schema(self):
        return {
            "type": "object",
            "properties": {
                "todos": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "content": {"type": "string"},
                            "activeForm": {"type": "string"},
                            "status": {"type": "string", "enum": ["pending", "in_progress", "completed"]}
                        },
                        "required": ["content", "activeForm", "status"]
                    }
                }
            },
            "required": ["todos"]
        }

    async def execute(self, **kwargs):
        # No-op: middleware handles everything
        return {"success": True}
```

### Тесты

**tests/middleware/test_todolist.py**:
```python
import pytest
from agentarea_agents_sdk.middleware.todolist import TodoListMiddleware

@pytest.mark.unit
async def test_todolist_creates_tasks():
    mw = TodoListMiddleware()
    state = {}

    tool_call = {
        'function': {
            'name': 'write_todos',
            'arguments': {
                'todos': [
                    {'content': 'Task 1', 'activeForm': 'Doing task 1', 'status': 'pending'},
                    {'content': 'Task 2', 'activeForm': 'Doing task 2', 'status': 'pending'}
                ]
            }
        }
    }

    result = await mw.before_tool_call(tool_call, state)

    assert state['todos'] is not None
    assert len(state['todos']) == 2
    assert 'id' in state['todos'][0]  # ID добавлен

@pytest.mark.unit
async def test_todolist_updates_status():
    mw = TodoListMiddleware()
    state = {}

    # Create
    tool_call = {
        'function': {
            'name': 'write_todos',
            'arguments': {
                'todos': [
                    {'content': 'Task 1', 'activeForm': 'Doing task 1', 'status': 'pending'}
                ]
            }
        }
    }
    await mw.before_tool_call(tool_call, state)
    task_id = state['todos'][0]['id']

    # Update
    tool_call['function']['arguments']['todos'][0]['id'] = task_id
    tool_call['function']['arguments']['todos'][0]['status'] = 'completed'
    await mw.before_tool_call(tool_call, state)

    # Verify in TaskService
    from uuid import UUID
    task = mw.task_service.get(UUID(task_id))
    assert task.status.value == 'completed'

@pytest.mark.integration
async def test_stateful_agent_with_todolist(skip_if_no_llm):
    skip_if_no_llm()

    from agentarea_agents_sdk.agents import StatefulAgent
    from agentarea_agents_sdk.middleware.todolist import TodoListMiddleware
    from agentarea_agents_sdk.tools.write_todos_tool import WriteTodosTool

    agent = StatefulAgent(
        name="TodoAgent",
        instruction="You plan tasks using write_todos tool",
        model_provider="ollama_chat",
        model_name="qwen2.5",
        middlewares=[TodoListMiddleware()],
        max_iterations=5
    )
    agent.add_tool(WriteTodosTool())

    result = await agent.run("Create a plan to make coffee")

    # Check state has todos
    assert agent.state.get('todos') is not None
    assert len(agent.state.get('todos')) > 0
```

### Как тестировать

```bash
# Unit (быстро)
pytest tests/middleware/test_todolist.py -m unit -v

# Integration (требует Ollama)
pytest tests/middleware/test_todolist.py -m integration -v

# Визуальный тест
python -c "
import asyncio
from agentarea_agents_sdk.agents import StatefulAgent
from agentarea_agents_sdk.middleware.todolist import TodoListMiddleware
from agentarea_agents_sdk.tools.write_todos_tool import WriteTodosTool

async def test():
    agent = StatefulAgent(
        name='TodoAgent',
        instruction='You plan tasks using write_todos tool',
        model_provider='ollama_chat',
        model_name='qwen2.5',
        middlewares=[TodoListMiddleware()],
        max_iterations=5
    )
    agent.add_tool(WriteTodosTool())

    async for chunk in agent.run_stream('Plan a simple web app'):
        print(chunk, end='', flush=True)

    print('\n\nFinal todos:', agent.state.get('todos'))

asyncio.run(test())
"
```

---

## PR #3: FilesystemMiddleware (3-4 дня)

### Что делаем

```
agentarea_agents_sdk/
└── middleware/
    └── filesystem.py    # FilesystemMiddleware + context eviction
```

### Код

**middleware/filesystem.py**:
```python
class FilesystemMiddleware:
    def __init__(self, eviction_threshold: int = 80_000):
        self.eviction_threshold = eviction_threshold

    async def before_llm_call(self, state: dict):
        # Initialize virtual FS
        if 'files' not in state:
            state['files'] = {}

    async def after_tool_call(self, tool_call: dict, result: Any, state: dict):
        # Context eviction для больших результатов
        if isinstance(result, str) and len(result) > self.eviction_threshold:
            file_path = f"/large_tool_results/{tool_call.get('id', 'unknown')}"
            state['files'][file_path] = result

            return {
                'evicted': True,
                'original_size': len(result),
                'file_path': file_path,
                'message': f"Result too large ({len(result)} chars). Saved to {file_path}. Use read_file() to access."
            }

        # Handle file operations
        tool_name = tool_call['function']['name']
        if tool_name == 'write_file':
            path = tool_call['function']['arguments']['file_name']
            content = tool_call['function']['arguments']['contents']
            state['files'][path] = content

        return result
```

### Тесты

**tests/middleware/test_filesystem.py**:
```python
import pytest

@pytest.mark.unit
async def test_context_eviction():
    from agentarea_agents_sdk.middleware.filesystem import FilesystemMiddleware

    mw = FilesystemMiddleware(eviction_threshold=100)
    state = {'files': {}}

    # Large result
    large_result = "x" * 150
    tool_call = {'id': 'test123', 'function': {'name': 'grep_search'}}

    result = await mw.after_tool_call(tool_call, large_result, state)

    assert result['evicted'] is True
    assert '/large_tool_results/test123' in state['files']
    assert state['files']['/large_tool_results/test123'] == large_result

@pytest.mark.unit
async def test_no_eviction_for_small_results():
    from agentarea_agents_sdk.middleware.filesystem import FilesystemMiddleware

    mw = FilesystemMiddleware(eviction_threshold=100)
    state = {'files': {}}

    small_result = "x" * 50
    tool_call = {'id': 'test123', 'function': {'name': 'grep_search'}}

    result = await mw.after_tool_call(tool_call, small_result, state)

    assert result == small_result  # Unchanged
    assert '/large_tool_results/test123' not in state['files']

@pytest.mark.integration
async def test_agent_with_filesystem(skip_if_no_llm):
    skip_if_no_llm()

    from agentarea_agents_sdk.agents import StatefulAgent
    from agentarea_agents_sdk.middleware.filesystem import FilesystemMiddleware
    from agentarea_agents_sdk.tools.file_toolset import FileToolset

    agent = StatefulAgent(
        name="FileAgent",
        instruction="You work with files",
        model_provider="ollama_chat",
        model_name="qwen2.5",
        middlewares=[FilesystemMiddleware()],
        max_iterations=5
    )
    agent.add_tool(FileToolset(base_dir="/tmp", enable_save=True))

    result = await agent.run("Save 'Hello World' to /workspace/test.txt")

    # Check virtual FS
    assert '/workspace/test.txt' in agent.state.get('files', {})
```

---

## PR #4: SummarizationMiddleware (5-7 дней)

### Что делаем

```
agentarea_agents_sdk/
├── middleware/
│   └── summarization.py
└── utils/
    └── token_counter.py
```

### Код

**utils/token_counter.py**:
```python
def count_tokens(text: str, model: str = "gpt-4") -> int:
    """Approximate token count."""
    # Simple approximation: 1 token ≈ 4 chars
    # TODO: use tiktoken for accurate counting
    return len(text) // 4

def count_messages_tokens(messages: list[dict]) -> int:
    total = 0
    for msg in messages:
        total += count_tokens(msg.get('content', ''))
    return total
```

**middleware/summarization.py**:
```python
from ..models.llm_model import LLMModel, LLMRequest
from ..utils.token_counter import count_messages_tokens

class SummarizationMiddleware:
    def __init__(self, max_tokens: int = 170_000, keep_last: int = 6):
        self.max_tokens = max_tokens
        self.keep_last = keep_last
        self.llm = None  # Will be set from agent

    def set_llm(self, llm: LLMModel):
        self.llm = llm

    async def before_llm_call(self, state: dict):
        messages = state.get('messages', [])

        if len(messages) < self.keep_last + 1:
            return  # Too few to summarize

        token_count = count_messages_tokens(messages)

        if token_count > self.max_tokens:
            # Summarize!
            recent = messages[-self.keep_last:]
            old = messages[:-self.keep_last]

            summary = await self._summarize(old)

            state['messages'] = [
                {'role': 'system', 'content': f'Previous context summary:\n{summary}'}
            ] + recent
            state['summarization_count'] = state.get('summarization_count', 0) + 1

    async def _summarize(self, messages: list[dict]) -> str:
        if not self.llm:
            # Fallback: simple concatenation
            return "Summary: " + " ".join(m.get('content', '')[:100] for m in messages)

        prompt = "Summarize this conversation, focusing on key decisions and context:\n\n"
        for msg in messages:
            prompt += f"{msg['role']}: {msg.get('content', '')}\n"

        request = LLMRequest(
            messages=[{'role': 'user', 'content': prompt}],
            temperature=0.3,
            max_tokens=2000
        )

        response = await self.llm.ainvoke(request)
        return response.content
```

### Тесты

**tests/middleware/test_summarization.py**:
```python
import pytest

@pytest.mark.unit
async def test_no_summarization_below_threshold():
    from agentarea_agents_sdk.middleware.summarization import SummarizationMiddleware

    mw = SummarizationMiddleware(max_tokens=1000, keep_last=2)
    state = {
        'messages': [
            {'role': 'user', 'content': 'short'},
            {'role': 'assistant', 'content': 'response'}
        ]
    }

    await mw.before_llm_call(state)

    assert len(state['messages']) == 2  # Unchanged

@pytest.mark.unit
async def test_summarization_above_threshold():
    from agentarea_agents_sdk.middleware.summarization import SummarizationMiddleware

    mw = SummarizationMiddleware(max_tokens=100, keep_last=2)

    # Create large messages
    state = {
        'messages': [
            {'role': 'user', 'content': 'x' * 1000},
            {'role': 'assistant', 'content': 'y' * 1000},
            {'role': 'user', 'content': 'z' * 1000},
            {'role': 'assistant', 'content': 'a' * 1000},
            {'role': 'user', 'content': 'recent1'},
            {'role': 'assistant', 'content': 'recent2'}
        ]
    }

    await mw.before_llm_call(state)

    # Should have: 1 summary + 2 recent
    assert len(state['messages']) == 3
    assert state['messages'][0]['role'] == 'system'
    assert 'summary' in state['messages'][0]['content'].lower()
    assert state['messages'][1]['content'] == 'recent1'
    assert state['summarization_count'] == 1
```

---

## PR #5: SubAgentMiddleware (7-10 дней)

### Код

**middleware/subagent.py**:
```python
class SubAgentMiddleware:
    def __init__(self, agent_factory=None, max_iterations: int = 50):
        self.agent_factory = agent_factory
        self.max_iterations = max_iterations

    async def before_tool_call(self, tool_call: dict, state: dict):
        if tool_call['function']['name'] == 'task':
            args = tool_call['function']['arguments']
            description = args['description']
            prompt = args.get('prompt', description)

            # Create isolated subagent
            subagent = self.agent_factory(
                instruction=prompt,
                max_iterations=self.max_iterations
            )

            # Run in isolation
            result = await subagent.run(description)

            # Store result
            state.setdefault('subagent_results', {})[tool_call['id']] = result

            # Return only result
            tool_call['_result'] = result

        return tool_call
```

### Тесты

**tests/middleware/test_subagent.py**:
```python
@pytest.mark.integration
async def test_subagent_isolation(skip_if_no_llm):
    skip_if_no_llm()

    from agentarea_agents_sdk.agents import StatefulAgent
    from agentarea_agents_sdk.middleware.subagent import SubAgentMiddleware

    def factory(**kwargs):
        return StatefulAgent(
            name="SubAgent",
            model_provider="ollama_chat",
            model_name="qwen2.5",
            **kwargs
        )

    agent = StatefulAgent(
        name="MainAgent",
        instruction="You delegate work",
        model_provider="ollama_chat",
        model_name="qwen2.5",
        middlewares=[SubAgentMiddleware(agent_factory=factory)],
        max_iterations=10
    )

    # Добавить task tool
    # ... (аналогично write_todos)

    result = await agent.run("Delegate: What is 2+2?")

    # Main agent должен иметь минимальный контекст
    # Subagent результат должен быть в state['subagent_results']
    assert 'subagent_results' in agent.state
```

---

## PR #6: ProgressTrackingMiddleware (3-4 дня)

### Код

**middleware/progress.py**:
```python
class ProgressTrackingMiddleware:
    def __init__(self, stagnation_threshold: int = 2):
        self.stagnation_threshold = stagnation_threshold
        self.last_completed_iteration = 0

    async def after_tool_call(self, tool_call: dict, result: Any, state: dict):
        # Check if any todo completed
        todos = state.get('todos', [])
        completed = [t for t in todos if t['status'] == 'completed']

        if len(completed) > self.last_completed_iteration:
            self.last_completed_iteration = state.get('iteration', 0)

        # Detect stagnation
        current_iteration = state.get('iteration', 0)
        if current_iteration - self.last_completed_iteration > self.stagnation_threshold:
            # Inject warning
            state['messages'].append({
                'role': 'system',
                'content': f'⚠️ Progress warning: {self.stagnation_threshold}+ iterations without completing a task. Consider revising your approach.'
            })

        return result
```

---

## E2E Integration Test (финальный)

**tests/test_full_integration.py**:
```python
import pytest

@pytest.mark.integration
async def test_full_stack(skip_if_no_llm):
    skip_if_no_llm()

    from agentarea_agents_sdk.agents import StatefulAgent
    from agentarea_agents_sdk.middleware import (
        TodoListMiddleware,
        FilesystemMiddleware,
        SummarizationMiddleware,
        ProgressTrackingMiddleware
    )

    agent = StatefulAgent(
        name="FullStackAgent",
        instruction="You are a systematic problem solver",
        model_provider="ollama_chat",
        model_name="qwen2.5",
        max_iterations=50,
        middlewares=[
            TodoListMiddleware(),
            FilesystemMiddleware(eviction_threshold=1000),
            SummarizationMiddleware(max_tokens=5000, keep_last=3),
            ProgressTrackingMiddleware(stagnation_threshold=3)
        ]
    )

    # Add all tools
    # ...

    result = await agent.run(
        "Create a plan to analyze a codebase, then execute it",
        success_criteria=[
            "Plan created",
            "Analysis completed",
            "Report generated"
        ]
    )

    # Verify
    assert agent.state.get('todos') is not None
    assert len([t for t in agent.state['todos'] if t['status'] == 'completed']) > 0
    assert agent.state.get('files') is not None
```

---

## Стратегия тестирования

### Unit тесты (быстрые)
- Каждый middleware изолированно
- Mock LLM responses
- Проверка state изменений
```bash
pytest -m unit -v
```

### Integration тесты (медленные)
- С реальным Ollama
- Простые задачи (2-5 итераций)
- Проверка end-to-end flow
```bash
pytest -m integration -v --slow
```

### Manual тесты
- Сложные задачи (10+ итераций)
- Визуальная проверка streaming
- Performance profiling
```bash
python examples/complex_task.py
```

---

## Метрики успеха

### Для каждого PR

1. **Unit tests**: >90% coverage для нового кода
2. **Integration test**: минимум 1 реальный LLM тест
3. **Backward compatibility**: старый Agent продолжает работать
4. **Performance**: overhead <10% для StatefulAgent без middleware

### Финальная интеграция

1. **E2E test**: задача 20+ итераций успешно
2. **Context management**: без overflow до 100 итераций
3. **Subagent isolation**: main agent context не растет от subagent работы
4. **Summarization**: корректно работает при 170k+ tokens

---

## Timeline

| PR | Компонент | Время | Критический путь |
|----|-----------|-------|------------------|
| #1 | Middleware infra | 3-5 дней | Да |
| #2 | TodoList | 3-4 дня | Да |
| #3 | Filesystem | 3-4 дня | Нет (параллельно #4) |
| #4 | Summarization | 5-7 дней | Да (нужен token counter) |
| #5 | SubAgent | 7-10 дней | Да (самый сложный) |
| #6 | Progress | 3-4 дня | Нет (параллельно #5) |
| #7 | E2E + docs | 3-5 дней | Да |

**Итого**: 27-39 дней (4-6 недель)

**Быстрый путь** (только критические): #1 → #2 → #4 → #5 → #7 = 21-31 день (3-4.5 недели)

---

## Быстрый старт

### Сегодня

```bash
cd Эксперименты/agentarea-agents-sdk

# Создать структуру
mkdir -p agentarea_agents_sdk/middleware
touch agentarea_agents_sdk/middleware/__init__.py
touch agentarea_agents_sdk/middleware/base.py
touch agentarea_agents_sdk/middleware/state.py

mkdir -p tests/middleware
touch tests/middleware/__init__.py
touch tests/middleware/test_base.py

# Создать ветку
git checkout -b feature/middleware-infrastructure

# Начать с Protocol
```

### Завтра

Реализовать base.py + state.py + тесты → PR #1

### Через неделю

TodoListMiddleware работает → PR #2

### Через месяц

Все 6 PR merged, full integration работает
